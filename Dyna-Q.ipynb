{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97d8e02-bfe6-4f7a-8aee-a4bf20ecd10b",
   "metadata": {},
   "source": [
    "## Задание "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78e8d2-df31-43c2-8488-88f0a0d4abf6",
   "metadata": {},
   "source": [
    "- Реализовать класс, моделирующий стохастическую среду Frozen Lake 8x8.\n",
    "- Обучить агента Dyna-Q, использующего стохастическую модель среды и сравнить с простым Q-агентом.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be48c12-d734-4871-8e75-d18bf221cc28",
   "metadata": {},
   "source": [
    "Для реализации задачи сначала создадим класс для моделирования стохастической среды Frozen Lake 8x8. Затем реализуем агента Dyna-Q и классического Q-learning агента для сравнения их эффективности. После этого визуализируем графики обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607292bf-aff8-45e9-9687-ca28d25d36a4",
   "metadata": {},
   "source": [
    "Импортируем все необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04fc7196-972f-4be7-907f-b0a379480efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7990e-1af1-4d9d-aada-9826a0b1f1a8",
   "metadata": {},
   "source": [
    "**Реализация Dyna-Q** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbdfa22-5f78-403d-97a2-fe1bfc950a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQAgent:\n",
    "    def __init__(self, env, alpha=0.3, gamma=0.99, epsilon=0.9, planning_steps=10):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.model = {}\n",
    "        self.planning_steps = planning_steps\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()  # случайное действие\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # наилучшее действие по Q-таблице\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        # Обновляем Q-таблицу на основе реального опыта\n",
    "        q_predict = self.q_table[state][action]\n",
    "        q_target = reward + self.gamma * np.max(self.q_table[next_state]) * (1 - done)\n",
    "        self.q_table[state][action] += self.alpha * (q_target - q_predict)\n",
    "\n",
    "        # Обновляем модель среды\n",
    "        self.model[(state, action)] = (reward, next_state, done)\n",
    "\n",
    "        # Планируем шаги на основе модели\n",
    "        for _ in range(self.planning_steps):\n",
    "            s, a = random.choice(list(self.model.keys()))\n",
    "            r, s_, d = self.model[(s, a)]\n",
    "            q_predict_model = self.q_table[s][a]\n",
    "            q_target_model = r + self.gamma * np.max(self.q_table[s_]) * (1 - d)\n",
    "            self.q_table[s][a] += self.alpha * (q_target_model - q_predict_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305437ff-e5e2-4f92-88a5-835c966bfa7a",
   "metadata": {},
   "source": [
    "**Реализация Q-learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b927783b-372b-4342-ab64-dba21b03fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.3, gamma=0.99, epsilon=0.9):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()  # случайное действие\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # наилучшее действие по Q-таблице\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        # Обновляем Q-таблицу на основе реального опыта\n",
    "        q_predict = self.q_table[state][action]\n",
    "        q_target = reward + self.gamma * np.max(self.q_table[next_state]) * (1 - done)\n",
    "        self.q_table[state][action] += self.alpha * (q_target - q_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7699bb2-6f29-473b-9366-e88eed882f7d",
   "metadata": {},
   "source": [
    "**Реализация функции обучения агента.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf64920-1eaa-4967-9924-ecd593ecad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_with_epsilon_decay(agent, episodes, epsilon_decay=0.99):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = agent.env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = agent.env.step(action)\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "        # Плавное уменьшение epsilon\n",
    "        agent.epsilon *= epsilon_decay\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a55b58-991f-4bfc-beb2-9a0b96196d12",
   "metadata": {},
   "source": [
    "**Создание среды FrozenLake 8x8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d33dfc-1472-4f5a-bf93-c2f3c49c2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40fb6ce-a38b-43fe-9add-50b0ee642002",
   "metadata": {},
   "source": [
    "**Создание агентов Dyna-Q и Q-learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5395c2d-0e6b-4563-8a99-a6378221456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyna_q_agent = DynaQAgent(env, alpha=0.3, gamma=0.99, epsilon=0.9, planning_steps=10)\n",
    "q_learning_agent = QLearningAgent(env, alpha=0.3, gamma=0.99, epsilon=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fea7c3-2fd0-4c23-a523-fd82eeebfbd8",
   "metadata": {},
   "source": [
    "**Обучение агентов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d887d8dd-9728-47f0-9e30-51c7778d6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10  # увеличиваем количество эпизодов для лучшего обучения\n",
    "dyna_q_rewards = train_agent_with_epsilon_decay(dyna_q_agent, episodes)\n",
    "q_learning_rewards = train_agent_with_epsilon_decay(q_learning_agent, episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2e7ff-bbcb-4a9d-81b6-ac484864b616",
   "metadata": {},
   "source": [
    "**Визуализация результатов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52be34-dd0d-4adf-bd8f-0cf1653ab2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dyna_q_rewards, label=\"Dyna-Q\")\n",
    "plt.plot(q_learning_rewards, label=\"Q-Learning\")\n",
    "plt.xlabel(\"эпизоды\")\n",
    "plt.ylabel(\"вознаграждение\")\n",
    "plt.legend()\n",
    "plt.title(\"Сравнение агентов\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940ddc6-dc2b-46f2-8ff0-077764219602",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60351e68-716a-4a28-b379-c1b7d7045686",
   "metadata": {},
   "source": [
    "Нами были изучены 2-а агента Dyna-Q и  Q-learning.  Испытания на своем железе показало не хватку ресурсов для корректного обучения и получания наивысшего вознаграждения и отображения его на графике. Обучение 100 эпиходов не дало нормальной картины на графике, для этого есть несколько причин- \n",
    "\n",
    "- Низкая вероятность успеха в среде\n",
    "- не найдены оптимальные параметры обучения. \n",
    "- Начальные условия Q-таблицы\n",
    "\n",
    "для того, что бы улучшить значения нужно:\n",
    "\n",
    "- увеличить количество эпох. \n",
    "- подобрать наилучшие параметры обучения. \n",
    "\n",
    "В моих реалиях это не наилучший вариант. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a205ac4-8c89-4eb5-ad05-b42a151dc771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772d6f3-3b00-4382-96df-74f2d70d6643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
